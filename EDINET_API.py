# -*- coding: utf-8 -*-
"""
Created on Thu Mar 28 15:34:50 2019
(-::-)P to ğŸ¤
edinet apiã‚’ä½¿ã„éå»ã®æ›¸é¡ä¸€è¦§ã®jsonåŒ–
ãƒ»ã©ã®ãã‚‰ã„ã®é »åº¦ã®requestsã‚’é€ã£ã¦è‰¯ã„ã‹ä¸æ˜
ãƒ»5å¹´åˆ†è©¦ã—ã¦ã¿ãŸã‚‰ç´„173M 40åˆ†
å‰æ—¥ã¾ã§ã®æ›¸é¡ä¸€è¦§ã‚’'xbrldocs.json'ã§ä¿å­˜
æ›¸é¡ä¸€è¦§ãŒæ—¥ä»˜æŒ‡å®šã§ã—ã‹å–ã‚Œãªã„ã®ã¯æ”¹å–„ã—ã¦æ¬²ã—ã„ãª
å‚è€ƒURL
http://d.hatena.ne.jp/xef/20121027/p2
https://note.nkmk.me/python-pandas-json-normalize/
https://note.mu/tom_programming/n/n45131a205717
http://73spica.tech/blog/requests-insecurerequestwarning-disable/
@author: Yusuke
"""

import pandas as pd
from datetime import date,timedelta
import requests
import urllib3
from urllib3.exceptions import InsecureRequestWarning
urllib3.disable_warnings(InsecureRequestWarning) #verify=Falseå¯¾ç­–
from itertools import chain
from time import sleep
import os
import pickle
from tqdm import tqdm

def request_json(sdt):    
    url = 'https://disclosure.edinet-fsa.go.jp/api/v1/documents.json'
    params = {'date': sdt, 'type': 2}
    headers = {'User-Agent': 'hoge'}
    res = requests.get(url, params=params, verify=False,timeout=3.5, headers=headers)
    sleep(1) #1ç§’é–“ã‚’ã‚ã‘ã‚‹
    rjson=res.json()
    return rjson
def datelog():
    #èª­è¾¼æ—¥ã‚’è¦šãˆã¦ãŠã
    datelogs=[] 
    if os.path.exists('datelog.pkl') :
        with open('datelog.pkl','rb') as log:
            datelogs=pickle.load(log)            
            datelogs.sort()        
    if(len(datelogs)>(1826)) : #éå»5å¹´ã‚’è¶…ãˆã‚‹æ—¥ä»˜ã¯å‰Šé™¤
        del datelogs[0:len(datelogs)-1826]                
    return datelogs

def json_docs(dt_5y,dt_today) :    
    doc_list=[]
    datelogs=datelog()  #éå»ã®èª­è¾¼æ—¥ã‚’å‘¼ã³å‡ºã™  
    pbar = tqdm(total=(dt_today-dt_5y).days)
    while(dt_5y < dt_today):
        sdt=dt_5y.strftime('%Y-%m-%d')        
        if sdt not in datelogs : #éå»ã«èª­ã¿è¾¼ã‚“ã æ—¥ä»˜ã¯èª­ã¿è¾¼ã¾ãªã„
            datelogs.append(sdt)
            rjson=request_json(sdt)
            if rjson['metadata']['status']=='200':
                rjson_list=rjson['results']
                if  rjson_list !=[] :
                    doc_list.append(rjson_list)                
        pbar.update(1)
        dt_5y=dt_5y+timedelta(days=1)
    
    pbar.close()      
    docs=list(chain.from_iterable(doc_list))  #flatten    
    df_doc2=pd.io.json.json_normalize(docs) #To Dataframe
    df_doc2=df_doc2.reset_index(drop=True)
    with open('datelog.pkl','wb') as log_file:
        datelogs.sort()
        pickle.dump(datelogs, log_file)
    return df_doc2
    
def get_xbrl(docID) :
    #æ›¸é¡å–å¾—
    url = 'https://disclosure.edinet-fsa.go.jp/api/v1/documents/'+docID
    params = { 'type': 2}
    headers = {'User-Agent': 'hoge'}
    res = requests.get(url, params=params,verify=False,timeout=3.5, headers=headers)
    sleep(1) #1ç§’é–“ã‚’ã‚ã‘ã‚‹
    contentType = res.headers['Content-Type']
    contentDisposition = res.headers['Content-Disposition']
    ATTRIBUTE = 'filename='
    fileName = contentDisposition[contentDisposition.find(ATTRIBUTE) + len(ATTRIBUTE):]
    print(contentType,fileName)

def concat_df(df_doc2):
    #concat
    if os.path.exists('xbrldocs.json') :  
        df_doc1=pd.read_json('xbrldocs.json')
        df_concat=pd.concat([df_doc1,df_doc2],sort=True)
        df_concat=df_concat.reset_index(drop=True)
        df_concat.to_json('xbrldocs.json')
    else : df_doc2.to_json('xbrldocs.json')
        
if __name__=='__main__':
    last_day=date.today()
    start_day=last_day-timedelta(days=8) #ç´„5å¹´å‰ãªã‚‰5*365ã€€æ­£ç¢ºã•ã‚’æ±‚ã‚ã‚‹ãªã‚‰relativedelta
    if last_day < start_day : start_day=last_day    
    df_doc2=json_docs(start_day,last_day) #éå»ï½æ—¥æ–‡ã®æ›¸é¡ãƒªã‚¹ãƒˆã‚’ã¾ã¨ã‚ã‚‹
    if df_doc2.empty==False:
        concat_df(df_doc2) 
    
'''    
    #docIDã‚’å–å¾—ã™ã‚‹
    df_read = pd.read_json('xbrldocs.json')
    docIDs=df_read['docID'].to_list()
    
    #æ›¸é¡å–å¾—
    get_xbrl(docIDs[0])
'''    
